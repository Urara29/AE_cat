{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLDuHIq3hHP2/S39eCODZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Urara29/AE_cat/blob/main/AE_cat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C2LUEF94UEiD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Conv2DTranspose, Activation, Flatten, Dropout, Reshape, GlobalAveragePooling2D, MaxPooling2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5rs_IIXVxpP",
        "outputId": "c860d183-71e9-42e6-e1c9-49c47e07220c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#カレントディレクトリに移動\n",
        "%cd \"/content/drive/My Drive/sotsuron/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Y_cWj5VyJO",
        "outputId": "76773d79-1cce-4f4c-85f7-63f0430e974e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/sotsuron\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install chromium-chromedriver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD99JmbZV-dv",
        "outputId": "43b375ed-637f-4a03-e699-c762bef4c805"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 95.1 MB of archives.\n",
            "After this operation, 319 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 107.0.5304.87-0ubuntu11.18.04.1 [1,158 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 107.0.5304.87-0ubuntu11.18.04.1 [83.1 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 107.0.5304.87-0ubuntu11.18.04.1 [5,260 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 107.0.5304.87-0ubuntu11.18.04.1 [5,570 kB]\n",
            "Fetched 95.1 MB in 8s (11.9 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 123991 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_107.0.5304.87-0ubuntu11.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_107.0.5304.87-0ubuntu11.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_107.0.5304.87-0ubuntu11.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_107.0.5304.87-0ubuntu11.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Setting up chromium-browser (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (107.0.5304.87-0ubuntu11.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install icrawler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9jZzSxHYLV4",
        "outputId": "ecfc60b5-0b28-4f5a-ba83-838e5d51a623"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: icrawler in /usr/local/lib/python3.7/dist-packages (0.6.6)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from icrawler) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from icrawler) (4.9.1)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from icrawler) (2.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from icrawler) (7.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from icrawler) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->icrawler) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->icrawler) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->icrawler) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->icrawler) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[In]\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "import os\n",
        "\n",
        "searchkeywords = '猫正面写真' # 検索するキーワード\n",
        "savedir = 'photos_cats' # 画像を保存するディレクトリ\n",
        "counts = 300 # 収集するデータ数\n",
        "\n",
        "def getimages(q=searchkeywords, maxnum=counts, save_dir=savedir):\n",
        "   if not os.path.exists(savedir): # ディレクトリがなければ作る\n",
        "       os.mkdir(savedir)\n",
        "       print(f'{savedir}フォルダを作成しました。')\n",
        "       \n",
        "   bing_crawler = BingImageCrawler(storage={'root_dir': save_dir})\n",
        "   bing_crawler.crawl(keyword=q, max_num=maxnum, filters={'type' : 'photo'}) #filtersでは画像のサイズやタイプを指定可能\n",
        "   \n",
        "getimages(q=searchkeywords, maxnum=counts, save_dir=savedir)   "
      ],
      "metadata": {
        "id": "B6I3cYA-WHUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "WH_RATIO = 1 / 1\n",
        "RESIZE_W = 256\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    for tgt in argv[1:]:\n",
        "        if os.path.isdir(tgt):\n",
        "            for f in os.listdir(tgt):\n",
        "                tp = os.path.join(tgt, f)\n",
        "                if not os.path.isdir(tp):\n",
        "                    edit_image(tp)\n",
        "        else:\n",
        "            edit_image(tgt)\n",
        "\n",
        "\n",
        "def edit_image(fp):\n",
        "    try:\n",
        "        img = Image.open(fp)\n",
        "        img_ratio = img.width / img.height\n",
        "\n",
        "        if img_ratio > WH_RATIO:\n",
        "            crop_pixel = (img.width - img.height * WH_RATIO) / 2\n",
        "            left = int(crop_pixel)\n",
        "            right = img.width - int(crop_pixel)\n",
        "            upper = 0\n",
        "            lower = img.height\n",
        "\n",
        "            img = img.crop((left, upper, right, lower))\n",
        "\n",
        "        if img.width > RESIZE_W:\n",
        "            img = img.resize((RESIZE_W, int(RESIZE_W * img.height / img.width)))\n",
        "\n",
        "        dst_dir = os.path.join(os.path.dirname(fp), 'trim')\n",
        "\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.mkdir(dst_dir)\n",
        "\n",
        "        img.save(os.path.join(dst_dir, os.path.basename(fp)))\n",
        "\n",
        "    except UnidentifiedImageError:\n",
        "        pass\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main(sys.argv)"
      ],
      "metadata": {
        "id": "XlmYBDBWWRhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#リサイズ処理\n",
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#パラメータ\n",
        "#======================================\n",
        "#画像を保存してあるフォルダ名\n",
        "f = 'photos_cats/'\n",
        "#リサイズした画像を保存するフォルダ名\n",
        "f_resize = 'cat_resize/'\n",
        "#リサイズ後のサイズ\n",
        "size = 256\n",
        "#======================================\n",
        "\n",
        "#処理\n",
        "#======================================\n",
        "if not os.path.isdir(f_resize):\n",
        "    os.makedirs(f_resize)\n",
        "files = os.listdir(f)\n",
        "for file in files:\n",
        "    img = Image.open(f + file).convert(\"RGBA\"); img.close\n",
        "    \n",
        "    tmp = np.array(img)\n",
        "    \n",
        "    mask = tmp[:,:,3] < 240\n",
        "    tmp[mask, 0] = 255\n",
        "    tmp[mask, 1] = 255\n",
        "    tmp[mask, 2] = 255\n",
        "    \n",
        "    img = Image.fromarray(tmp[:,:,0:3])\n",
        "    \n",
        "    width, height = img.size\n",
        "    if width == height:\n",
        "        tmp = img\n",
        "    elif width > height:\n",
        "        tmp = Image.new('RGB', (width, width), (255, 255, 255))\n",
        "        tmp.paste(img, (0, (width - height) // 2))\n",
        "    else:\n",
        "        tmp = Image.new('RGB', (height, height), (255, 255, 255))\n",
        "        tmp.paste(img, ((height - width) // 2, 0))\n",
        "    img_resize = tmp.resize((size, size), Image.BICUBIC)\n",
        "    img_resize.save(f_resize + file)\n",
        "    print(\"リサイズ完了\")\n",
        "print()\n",
        "#======================================"
      ],
      "metadata": {
        "id": "7s9HgJ5QWXIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#パラメータ\n",
        "\n",
        "#教師画像の親フォルダ\n",
        "f = '/content/drive/MyDrive/sotsuron/cat_resize/'\n",
        "\n",
        "#ミニバッチサイズ（教師データ数の公約数にしてください）\n",
        "batch_size = 40\n",
        "\n",
        "#乱数列の次元\n",
        "z_dim = 100\n",
        "\n",
        "#discriminatorの学習率\n",
        "opt = keras.optimizers.Adam(lr=0.0002)\n",
        "\n",
        "#画像を保存するフォルダ\n",
        "img_f = '/content/drive/MyDrive/sotsuron/AE_img/'\n",
        "#重みを保存するフォルダ\n",
        "para_f = '/content/drive/MyDrive/sotsuron/AE_para/'"
      ],
      "metadata": {
        "id": "0bEF7RFDWd5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#教師データ読み込み\n",
        "x_train = []\n",
        "files = os.listdir(f)\n",
        "for file in files:\n",
        "    img = Image.open(f + file).convert(\"RGB\"); img.close\n",
        "    x_train.append(np.array(img))\n",
        "x_train = np.array(x_train)\n",
        "\n",
        "#-1～+1に規格化\n",
        "x_train = (x_train - 127.5) / 127.5\n",
        "\n",
        "#確認用に手前の10枚を隔離\n",
        "x_check = deepcopy(x_train[:10])\n",
        "x_train = x_train[10:]\n",
        "\n",
        "#入力と出力は同じですよ\n",
        "y_train = x_train\n",
        "\n",
        "print('枚数, たて, よこ, チャンネル')\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "Ua5GHmHMWm21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの定義\n",
        "\n",
        "def encoder_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    #128*128*3ch → 64*64*32chにたたむ\n",
        "    model.add(Conv2D(32, (4, 4), strides=(2, 2), padding='same', input_shape=(128, 128, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #64*64*32ch → 32*32*64chにたたむ\n",
        "    model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #32*32*64ch → 16*16*128chにたたむ\n",
        "    model.add(Conv2D(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #16*16*128ch → 8*8*256chにたたむ\n",
        "    model.add(Conv2D(256, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #フラットに伸ばして\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    #100次元まで圧縮\n",
        "    model.add(Dense(z_dim))\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def decoder_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    \n",
        "    #100次元 → 8*8*256=16384次元に展開\n",
        "    model.add(Dense(8*8*256, input_shape=(z_dim, )))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #8*8*256chに変形\n",
        "    model.add(Reshape((8, 8, 256)))\n",
        "    model.add(Dropout(0.5))\n",
        "    \n",
        "    #8*8*256ch → 16*16*128chにアップ\n",
        "    model.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #16*16*128ch → 32*32*64chにアップ\n",
        "    model.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #32*32*64ch → 64*64*32chにアップ\n",
        "    model.add(Conv2DTranspose(32, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    \n",
        "    #64*64*32ch → 128*128*3chにアップ\n",
        "    model.add(Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def autoencoder_model(encoder, decoder):\n",
        "    model = Sequential()\n",
        "    model.add(encoder)\n",
        "    model.add(decoder)\n",
        "    return model"
      ],
      "metadata": {
        "id": "21yqhc5cWsh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの生成\n",
        "\n",
        "#encoderの生成\n",
        "encoder = encoder_model()\n",
        "encoder.summary()\n",
        "#decoderの生成\n",
        "decoder = decoder_model()\n",
        "decoder.summary()\n",
        "#autoencoderの作成\n",
        "autoencoder = autoencoder_model(encoder, decoder)\n",
        "autoencoder.summary()\n",
        "\n",
        "#autoencoderのコンパイル\n",
        "autoencoder.compile(loss='msle', optimizer=opt)"
      ],
      "metadata": {
        "id": "JyCxMKVtWv3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#保存用フォルダ作成\n",
        "if not os.path.isdir(para_f): \n",
        "    os.makedirs(para_f)\n",
        "if not os.path.isdir(img_f): \n",
        "    os.makedirs(img_f)"
      ],
      "metadata": {
        "id": "k_h7cV8UWx7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#学習\n",
        "hist = autoencoder.fit(x_train, y_train, epochs=30000, batch_size=batch_size, verbose=2, callbacks=[EpisodeLogger()])"
      ],
      "metadata": {
        "id": "jeOC1KqIXACR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}